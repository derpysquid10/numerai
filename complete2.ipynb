{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading prediction data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'vv4.3/validation_int8.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m feature_sets \u001b[38;5;241m=\u001b[39m feature_metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_sets\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     27\u001b[0m feature_set \u001b[38;5;241m=\u001b[39m feature_sets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 30\u001b[0m train, validation, live_features \u001b[38;5;241m=\u001b[39m \u001b[43mu2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_prediction_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# train = train[train[\"era\"].isin(train[\"era\"].unique()[::4])]\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# validation = validation[validation[\"era\"].isin(validation[\"era\"].unique()[::4])]\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Defining Model\u001b[39;00m\n\u001b[1;32m     39\u001b[0m model \u001b[38;5;241m=\u001b[39m CatBoostRegressor(\n\u001b[1;32m     40\u001b[0m   n_estimators\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     41\u001b[0m   learning_rate\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.00000388\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m   verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     46\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/numerai/utils_plus2.py:26\u001b[0m, in \u001b[0;36mread_prediction_data\u001b[0;34m(features, training_data_path, validation_data_path, live_feature_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# read in just those features along with era and target columns\u001b[39;00m\n\u001b[1;32m     22\u001b[0m training_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(training_data_path, \n\u001b[1;32m     23\u001b[0m                                 columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mera\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m features\n\u001b[1;32m     24\u001b[0m                                 )\n\u001b[0;32m---> 26\u001b[0m validation_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation_data_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mera\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m live_features \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(live_feature_path,\n\u001b[1;32m     31\u001b[0m                                 columns\u001b[38;5;241m=\u001b[39mfeatures\n\u001b[1;32m     32\u001b[0m                                 )\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m training_data, validation_data, live_features\n",
      "File \u001b[0;32m~/anaconda3/envs/numerai8/lib/python3.10/site-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/numerai8/lib/python3.10/site-packages/pandas/io/parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    275\u001b[0m         path_or_handle,\n\u001b[1;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    280\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/numerai8/lib/python3.10/site-packages/pandas/io/parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/numerai8/lib/python3.10/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vv4.3/validation_int8.parquet'"
     ]
    }
   ],
   "source": [
    "# Initialize NumerAPI - the official Python API client for Numerai\n",
    "!pip install -q --no-deps numerai-tools\n",
    "\n",
    "from numerapi import NumerAPI\n",
    "import json \n",
    "import pandas as pd\n",
    "import time\n",
    "from catboost import CatBoostRegressor\n",
    "from numerai_tools.scoring import numerai_corr, correlation_contribution\n",
    "\n",
    "import utils_plus2 as u2\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "napi = NumerAPI()\n",
    "\n",
    "###################################\n",
    "model_name = 'July25_10'\n",
    "DATA_VERSION = \"v4.3\"\n",
    "###################################\n",
    "\n",
    "\n",
    "# Feature Stuff\n",
    "feature_metadata = json.load(open(f\"{DATA_VERSION}/features.json\"))\n",
    "feature_sets = feature_metadata[\"feature_sets\"]\n",
    "feature_set = feature_sets[\"all\"]\n",
    "\n",
    "\n",
    "train, validation, live_features = u2.read_prediction_data(feature_set)\n",
    "\n",
    "\n",
    "# train = train[train[\"era\"].isin(train[\"era\"].unique()[::4])]\n",
    "# validation = validation[validation[\"era\"].isin(validation[\"era\"].unique()[::4])]\n",
    "\n",
    "\n",
    "\n",
    "# Defining Model\n",
    "model = CatBoostRegressor(\n",
    "  n_estimators= 10,\n",
    "  learning_rate= 0.00000388,\n",
    "  task_type = \"GPU\",\n",
    "  depth = 13,\n",
    "  l2_leaf_reg = 9,\n",
    "  verbose = False\n",
    ")\n",
    "\n",
    "# Training\n",
    "model.fit(\n",
    "  train[feature_set],\n",
    "  train[\"target\"]\n",
    ")\n",
    "\n",
    "model.save_model(f'models/{model_name}.txt')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################## VALIDATION ####################\n",
    "\n",
    "\n",
    "\n",
    "validation = validation[validation[\"data_type\"] == \"validation\"]\n",
    "del validation[\"data_type\"]\n",
    "\n",
    "# Downsample to every 4th era to reduce memory usage and speedup evaluation (suggested for Colab free tier)\n",
    "# Comment out the line below to use all the data (slower and higher memory usage, but more accurate evaluation)\n",
    "\n",
    "# Eras are 1 week apart, but targets look 20 days (o 4 weeks/eras) into the future,\n",
    "# so we need to \"embargo\" the first 4 eras following our last train era to avoid \"data leakage\"\n",
    "last_train_era = int(train[\"era\"].unique()[-1])\n",
    "eras_to_embargo = [str(era).zfill(4) for era in [last_train_era + i for i in range(4)]]\n",
    "validation = validation[~validation[\"era\"].isin(eras_to_embargo)]\n",
    "\n",
    "# Generate predictions against the out-of-sample validation features\n",
    "# This will take a few minutes üçµ\n",
    "validation[\"prediction\"] = model.predict(validation[feature_set])\n",
    "validation[[\"era\", \"prediction\", \"target\"]]\n",
    "\n",
    "\n",
    "# Download and join in the meta_model for the validation eras\n",
    "# napi.download_dataset(f\"{DATA_VERSION}/meta_model.parquet\")\n",
    "validation[\"meta_model\"] = pd.read_parquet(\n",
    "    f\"{DATA_VERSION}/meta_model.parquet\"\n",
    ")[\"numerai_meta_model\"]\n",
    "\n",
    "# Compute the per-era corr between our predictions and the target values\n",
    "per_era_corr = validation.groupby(\"era\").apply(\n",
    "    lambda x: numerai_corr(x[[\"prediction\"]].dropna(), x[\"target\"].dropna())\n",
    ")\n",
    "\n",
    "# Compute the per-era mmc between our predictions, the meta model, and the target values\n",
    "per_era_mmc = validation.dropna().groupby(\"era\").apply(\n",
    "    lambda x: correlation_contribution(x[[\"prediction\"]], x[\"meta_model\"], x[\"target\"])\n",
    ")\n",
    "\n",
    "\n",
    "# Plot the per-era correlation\n",
    "per_era_corr.plot(\n",
    "  title=\"Validation CORR\",\n",
    "  kind=\"bar\",\n",
    "  figsize=(8, 4),\n",
    "  xticks=[],\n",
    "  legend=False,\n",
    "  snap=False\n",
    ")\n",
    "per_era_mmc.plot(\n",
    "  title=\"Validation MMC\",\n",
    "  kind=\"bar\",\n",
    "  figsize=(8, 4),\n",
    "  xticks=[],\n",
    "  legend=False,\n",
    "  snap=False\n",
    ")\n",
    "\n",
    "# Plot the cumulative per-era correlation\n",
    "per_era_corr.cumsum().plot(\n",
    "  title=\"Cumulative Validation CORR\",\n",
    "  kind=\"line\",\n",
    "  figsize=(8, 4),\n",
    "  legend=False\n",
    ")\n",
    "per_era_mmc.cumsum().plot(\n",
    "  title=\"Cumulative Validation MMC\",\n",
    "  kind=\"line\",\n",
    "  figsize=(8, 4),\n",
    "  legend=False\n",
    ")\n",
    "\n",
    "# Compute performance metrics\n",
    "corr_mean = per_era_corr.mean()\n",
    "corr_std = per_era_corr.std(ddof=0)\n",
    "corr_sharpe = corr_mean / corr_std\n",
    "corr_max_drawdown = (per_era_corr.cumsum().expanding(min_periods=1).max() - per_era_corr.cumsum()).max()\n",
    "\n",
    "mmc_mean = per_era_mmc.mean()\n",
    "mmc_std = per_era_mmc.std(ddof=0)\n",
    "mmc_sharpe = mmc_mean / mmc_std\n",
    "mmc_max_drawdown = (per_era_mmc.cumsum().expanding(min_periods=1).max() - per_era_mmc.cumsum()).max()\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"mean\": [corr_mean, mmc_mean],\n",
    "    \"std\": [corr_std, mmc_std],\n",
    "    \"sharpe\": [corr_sharpe, mmc_sharpe],\n",
    "    \"max_drawdown\": [corr_max_drawdown, mmc_max_drawdown]\n",
    "}, index=[\"CORR\", \"MMC\"]).T\n",
    "\n",
    "\n",
    "# Compute performance metrics\n",
    "corr_mean = per_era_corr.mean()\n",
    "corr_std = per_era_corr.std(ddof=0)\n",
    "corr_sharpe = corr_mean / corr_std\n",
    "corr_max_drawdown = (per_era_corr.cumsum().expanding(min_periods=1).max() - per_era_corr.cumsum()).max()\n",
    "\n",
    "mmc_mean = per_era_mmc.mean()\n",
    "mmc_std = per_era_mmc.std(ddof=0)\n",
    "mmc_sharpe = mmc_mean / mmc_std\n",
    "mmc_max_drawdown = (per_era_mmc.cumsum().expanding(min_periods=1).max() - per_era_mmc.cumsum()).max()\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"mean\": [corr_mean, mmc_mean],\n",
    "    \"std\": [corr_std, mmc_std],\n",
    "    \"sharpe\": [corr_sharpe, mmc_sharpe],\n",
    "    \"max_drawdown\": [corr_max_drawdown, mmc_max_drawdown]\n",
    "}, index=[\"CORR\", \"MMC\"]).T\n",
    "\n",
    "#######################################################################\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "time_elapsed_mins = (end_time - start_time)//60\n",
    "time_elapsed_secs = (end_time - start_time)%60\n",
    "print(f\"Time elapsed: {time_elapsed_mins} mins {time_elapsed_secs} secs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CORR</th>\n",
       "      <th>MMC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>prediction    0.029739\n",
       "dtype: float64</td>\n",
       "      <td>prediction    0.009011\n",
       "dtype: float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>prediction    0.022116\n",
       "dtype: float64</td>\n",
       "      <td>prediction    0.015904\n",
       "dtype: float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sharpe</th>\n",
       "      <td>prediction    1.344653\n",
       "dtype: float64</td>\n",
       "      <td>prediction    0.566597\n",
       "dtype: float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_drawdown</th>\n",
       "      <td>prediction    0.03984\n",
       "dtype: float64</td>\n",
       "      <td>prediction    0.047168\n",
       "dtype: float64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               CORR  \\\n",
       "mean          prediction    0.029739\n",
       "dtype: float64   \n",
       "std           prediction    0.022116\n",
       "dtype: float64   \n",
       "sharpe        prediction    1.344653\n",
       "dtype: float64   \n",
       "max_drawdown   prediction    0.03984\n",
       "dtype: float64   \n",
       "\n",
       "                                                MMC  \n",
       "mean          prediction    0.009011\n",
       "dtype: float64  \n",
       "std           prediction    0.015904\n",
       "dtype: float64  \n",
       "sharpe        prediction    0.566597\n",
       "dtype: float64  \n",
       "max_drawdown  prediction    0.047168\n",
       "dtype: float64  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute performance metrics\n",
    "corr_mean = per_era_corr.mean()\n",
    "corr_std = per_era_corr.std(ddof=0)\n",
    "corr_sharpe = corr_mean / corr_std\n",
    "corr_max_drawdown = (per_era_corr.cumsum().expanding(min_periods=1).max() - per_era_corr.cumsum()).max()\n",
    "\n",
    "mmc_mean = per_era_mmc.mean()\n",
    "mmc_std = per_era_mmc.std(ddof=0)\n",
    "mmc_sharpe = mmc_mean / mmc_std\n",
    "mmc_max_drawdown = (per_era_mmc.cumsum().expanding(min_periods=1).max() - per_era_mmc.cumsum()).max()\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"mean\": [corr_mean, mmc_mean],\n",
    "    \"std\": [corr_std, mmc_std],\n",
    "    \"sharpe\": [corr_sharpe, mmc_sharpe],\n",
    "    \"max_drawdown\": [corr_max_drawdown, mmc_max_drawdown]\n",
    "}, index=[\"CORR\", \"MMC\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 20:16:41,407 INFO numerapi.utils: target file already exists\n",
      "2024-07-24 20:16:41,409 INFO numerapi.utils: download complete\n"
     ]
    }
   ],
   "source": [
    "################## LIVE PREDICTION #########################\n",
    "model = CatBoostRegressor()\n",
    "model.load_model(f'models/20.txt')\n",
    "\n",
    "# Download latest live features\n",
    "napi.download_dataset(f\"{DATA_VERSION}/live_int8.parquet\")\n",
    "\n",
    "# Load live features\n",
    "live_features = pd.read_parquet(f\"{DATA_VERSION}/live_int8.parquet\", columns=feature_set)\n",
    "\n",
    "# Generate live predictions\n",
    "live_predictions = model.predict(live_features[feature_set])\n",
    "\n",
    "# Format submission\n",
    "a = pd.Series(live_predictions, index=live_features.index).to_frame(\"prediction\")\n",
    "a.to_csv(f\"predictions2/july24_3.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numerai8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
