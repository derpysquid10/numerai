{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install halo\n",
    "!pip install numerapi\n",
    "!pip install catboost\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from halo import Halo\n",
    "from pathlib import Path\n",
    "import json\n",
    "from scipy.stats import skew\n",
    "import numerapi\n",
    "import gc\n",
    "from numerapi import NumerAPI\n",
    "import time\n",
    "\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "import catboost\n",
    "\n",
    "\n",
    "from utils_plus import *\n",
    "\n",
    "#download_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model_name = \"catboost2_small\"\n",
    "\n",
    "with open(\"./data/features.json\", \"r\") as f:\n",
    "    feature_metadata = json.load(f)\n",
    "features = feature_metadata[\"feature_sets\"][\"fncv3_features\"]\n",
    "\n",
    "# read the training and validation data given the predefined features stored in parquets as pandas DataFrames\n",
    "training_data, validation_data = read_learning_data(features)\n",
    "# extract feature matrix and target vector used for training\n",
    "X_train = training_data.filter(like='feature_', axis='columns')\n",
    "y_train = training_data[TARGET_COL]\n",
    "# extract feature matrix and target vector used for validation\n",
    "X_val = validation_data.filter(like='feature_', axis='columns')\n",
    "y_val = validation_data[TARGET_COL]\n",
    "# \"garbage collection\" (gc) gets rid of unused data and frees up memory\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feature_neutralization = 40  # parameter for feature neutralization used after we make our predictions\n",
    "params ={\"n_estimators\": 11000,\n",
    "          \"learning_rate\": 0.003,\n",
    "          \"task_type\":\"GPU\",\n",
    "          \"depth\": 9,\n",
    "          'l2_leaf_reg': 9}\n",
    "\n",
    "model = CatBoostRegressor(**params)\n",
    "\n",
    "\n",
    "spinner.start('Training model')\n",
    "model.fit(X_train, y_train, early_stopping_rounds = 10, verbose = False)\n",
    "spinner.succeed()\n",
    "gc.collect()\n",
    "spinner.start('Saving model')\n",
    "model.save_model(f'models/{model_name}.txt')\n",
    "spinner.succeed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” Saving model\n",
      "Exporting Predictions to csv...\n",
      "Time elapsed: 5.0 mins 54.7894024848938 secs\n",
      "Done!\n",
      "Error in callback <function Halo.__init__.<locals>.clean_up at 0x7f5f0134d790> (for post_run_cell), with arguments args (<ExecutionResult object at 7f5f0132d190, execution_count=10 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7f5f0132d220, raw_cell=\"######## Delete belowwhen done using diagnostics t..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/home/gordontan/Desktop/numerai/yup.ipynb#W3sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "clean_up() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: clean_up() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "######## Delete belowwhen done using diagnostics tool\n",
    "validation_data.loc[:, f\"preds_{model_name}\"] = model.predict(X_val)\n",
    "spinner.succeed()\n",
    "gc.collect()\n",
    "neutralize_riskiest_features(training_data, validation_data, features, model_name, k=num_feature_neutralization)\n",
    "\n",
    "print('Exporting Predictions to csv...')\n",
    "validation_data[\"prediction\"] = validation_data[f\"preds_{model_name}_neutral_riskiest_{num_feature_neutralization}\"] \\\n",
    "    .rank(pct=True)\n",
    "validation_data[\"prediction\"].to_csv(f\"predictions/{model_name}.csv\")\n",
    "\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "time_elapsed_mins = (end_time - start_time)//60\n",
    "time_elapsed_secs = (end_time - start_time)%60\n",
    "print(f\"Time elapsed: {time_elapsed_mins} mins {time_elapsed_secs} secs\")\n",
    "\n",
    "\n",
    "\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
