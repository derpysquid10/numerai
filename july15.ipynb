{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q numerapi pandas pyarrow matplotlib lightgbm scikit-learn cloudpickle scipy==1.10.1\n",
    "!pip install halo\n",
    "# Inline plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NumerAPI - the official Python API client for Numerai\n",
    "from numerapi import NumerAPI\n",
    "napi = NumerAPI()\n",
    "\n",
    "# list the datasets and available versions\n",
    "all_datasets = napi.list_datasets()\n",
    "dataset_versions = list(set(d.split('/')[0] for d in all_datasets))\n",
    "print(\"Available versions:\\n\", dataset_versions)\n",
    "\n",
    "# Set data version to one of the latest datasets\n",
    "DATA_VERSION = \"v4.3\"\n",
    "\n",
    "# Print all files available for download for our version\n",
    "current_version_files = [f for f in all_datasets if f.startswith(DATA_VERSION)]\n",
    "print(\"availbable\", DATA_VERSION, \"files:\\n\", current_version_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# download the feature metadata file\n",
    "napi.download_dataset(f\"{DATA_VERSION}/features.json\");\n",
    "\n",
    "# read the metadata and display\n",
    "feature_metadata = json.load(open(f\"{DATA_VERSION}/features.json\"))\n",
    "for metadata in feature_metadata:\n",
    "  print(metadata, len(feature_metadata[metadata]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = feature_metadata[\"feature_sets\"]\n",
    "for feature_set in [\"small\", \"medium\", \"all\"]:\n",
    "  print(feature_set, len(feature_sets[feature_set]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define our feature set\n",
    "feature_set = feature_sets[\"medium\"]\n",
    "\n",
    "# Download the training data - this will take a few minutes\n",
    "napi.download_dataset(f\"{DATA_VERSION}/train_int8.parquet\");\n",
    "\n",
    "# Load only the \"medium\" feature set to\n",
    "# Use the \"all\" feature set to use all features\n",
    "train = pd.read_parquet(\n",
    "    f\"{DATA_VERSION}/train_int8.parquet\",\n",
    "    columns=[\"era\", \"target\"] + feature_set\n",
    ")\n",
    "\n",
    "# Downsample to every 4th era to reduce memory usage and speedup model training (suggested for Colab free tier)\n",
    "# Comment out the line below to use all the data\n",
    "train = train[train[\"era\"].isin(train[\"era\"].unique()[::4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: Verbose\n",
      "[LightGBM] [Warning] Unknown parameter: Verbose\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004423 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3525\n",
      "[LightGBM] [Info] Number of data points in the train set: 606176, number of used features: 705\n",
      "[LightGBM] [Info] Start training from score 0.499979\n",
      "Error in callback <function Halo.__init__.<locals>.clean_up at 0x7f93c082b740> (for post_run_cell), with arguments args (<ExecutionResult object at 7f9225781f10, execution_count=16 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7f93c0aac310, raw_cell=\"# https://lightgbm.readthedocs.io/en/latest/python..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/home/gordontan/Desktop/numerai/july15.ipynb#W5sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "# https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html\n",
    "import lightgbm as lgb\n",
    "\n",
    "# https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
    "model = lgb.LGBMRegressor(\n",
    "  n_estimators=2000,\n",
    "  learning_rate=0.01,\n",
    "  max_depth=5,\n",
    "  # task_type=\"GPU\",\n",
    "  num_leaves=2**5-1,\n",
    "  colsample_bytree=0.1,\n",
    "  Verbose = True\n",
    "  \n",
    ")\n",
    "\n",
    "# This will take a few minutes 🍵\n",
    "model.fit(\n",
    "  train[feature_set],\n",
    "  train[\"target\"]\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest live features\n",
    "napi.download_dataset(f\"{DATA_VERSION}/live_int8.parquet\")\n",
    "\n",
    "# Load live features\n",
    "live_features = pd.read_parquet(f\"{DATA_VERSION}/live_int8.parquet\", columns=feature_set)\n",
    "\n",
    "# Generate live predictions\n",
    "live_predictions = model.predict(live_features[feature_set])\n",
    "\n",
    "# Format submission\n",
    "pd.Series(live_predictions, index=live_features.index).to_frame(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your prediction pipeline as a function\n",
    "def predict(live_features: pd.DataFrame) -> pd.DataFrame:\n",
    "    live_predictions = model.predict(live_features[feature_set])\n",
    "    submission = pd.Series(live_predictions, index=live_features.index)\n",
    "    return submission.to_frame(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the cloudpickle library to serialize your function\n",
    "import cloudpickle\n",
    "p = cloudpickle.dumps(predict)\n",
    "with open(\"predict.pkl\", \"wb\") as f:\n",
    "    f.write(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download file if running in Google Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('predict.pkl')\n",
    "except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
